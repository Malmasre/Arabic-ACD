# Arabic-ACD
The uesd Datasets:
The HARD data set was obtained from this Github repo (https://github.com/elnagara/HARD-Arabic-Dataset) where you can find detailed statistics about it. Note that we used only the `unbalanced-reviews` version.Further Info on the SemEval data set can be found in these links:
Data and Tools of SemEval 2016 task 5 workshop (https://alt.qcri.org/semeval2016/task5/index.php?id=data-and-tools)
SemEval 2016 Arabic Hotel training set (http://metashare.ilsp.gr:8080/repository/browse/semeval-2016-absa-hotels-reviews-arabic-train-data-subtask-1/7e17228c5d2c11e58908842b2b6a04d71281285461104837bd7bbbafd99bb929/) 
SemEval 2016 Arabic Hotel Gold Test set (http://metashare.ilsp.gr:8080/repository/browse/semeval-2016-absa-hotel-reviews-arabic-test-data-gold-subtask-1/f41dc10ed17911e585c4842b2b6a04d79e7743f9a46e4d6cb45228af9ee8ffe4/)

The methdology:
1.	Simplified, a first step in this methodology involves the processing of the inputs from the labelled SemEval 2016 dataset to train a teacher model utilizing a standard cross entropy loss.
2.	The second step entails generating pseudo labels on the unlabelled HARD dataset using the previously developed teacher model. It is important to note that we did not add noise to the teacher model at this stage.
3.	In the third step of this process, a noised student model (that is equal in size to the previously developed teacher) is trained on the combined datasets with the objective of minimizing the combined cross entropy loss. Model noise is introduced into the student so that it does not only learn the teacherâ€™s knowledge but goes beyond it. The noise parameters implemented are specifically dropout and stochastic depth function, to the exclusion of noising the data through augmentation, which is considered input noise. Using dropout removes dropout neurons from the neural network (AraBERT). As this is repeated for each training example, we end up with different models. The probabilities of each of those models are later averaged during the testing stage, which is similar to an ensembling results. Of significance to our research, and for purposes of comparison with the ImageNet 2012 used in [39], where it is assumed that a balanced dataset has a positive effect on the performance of the student model, it must be observed that our unlabelled dataset, HARD 2016, is not balanced.
4.	Diverging from the fourth step described in [39], which suggests an iteration process where student becomes teacher to generate pseudo labels on the unlablled dataset, we opted for implementing an ensembling technique. The objective has been to utilize the moderately better performance of the student model in combination with the teacher. The input data of the combined datasets is run through both student and teacher models resulting in different probabilities, which are ultimately averaged.
